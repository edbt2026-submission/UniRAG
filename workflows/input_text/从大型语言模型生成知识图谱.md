# Generating Knowledge Graphs from Large Language Models: A Comparative Study of GPT-4, LLaMA 2, and BERT  

$1^{\mathrm{st}}$    Ahan Bhatt  $2^{\mathrm{nd}}$    Nandan Vaghela  $3^{\mathrm{rd}}$    Kush Dudhia IITE IITE IITE Indus University Indus University Indus University Ahmedabad, Gujarat, India Ahmedabad, Gujarat, India Ahmedabad, Gujarat, India bhattahan  $@$  gmail.com nanda nva g he la.20.ce  $@$  iite.indusuni.ac.in kushdudhia.20.ce@iite.indusuni.ac.in  

Abstract —Knowledge Graphs (KGs) are essential for the functionality of GraphRAGs, a form of Retrieval-Augmented Generative Systems (RAGs) that excel in tasks requiring structured reasoning and semantic understanding. However, creating KGs for GraphRAGs remains a significant challenge due to accuracy and s cal ability limitations of traditional methods. This paper introduces a novel approach leveraging large language models (LLMs) like GPT-4, LLaMA 2 (13B), and BERT to generate KGs directly from unstructured data, bypassing traditional pipelines. Using metrics such as Precision, Recall, F1-Score, Graph Edit Distance, and Semantic Similarity, we evaluate the models’ ability to generate high-quality KGs. Results demonstrate that GPT4 achieves superior semantic fidelity and structural accuracy, LLaMA 2 excels in lightweight, domain-specific graphs, and BERT provides insights into challenges in entity-relationship modeling. This study underscores the potential of LLMs to streamline KG creation and enhance GraphRAG accessibility for real-world applications, while setting a foundation for future advancements.  

Index Terms —Knowledge Graph Generation, GraphRAGs, Large Language Models, Automated Knowledge Extraction  

# I. I N TRO DUCTION  

Knowledge Graphs (KGs) have become essential tools for organizing and representing complex relationships within data, serving as the backbone for numerous AI systems. Among these, GraphRAGs (Retrieval-Augmented Generative Systems) are regarded as one of the most effective approaches for combining structured knowledge with generative capabilities. However, the manual creation of KGs for GraphRAGs remains a significant challenge. Traditional techniques, such as relationship classification, require considerable effort and often lack the precision needed to handle intricate or large-scale datasets.  

To address this, we explore a method that leverages Large Language Models (LLMs) to automate the KG generation process, making it more accessible and efficient. The primary goal of this research is to simplify the creation of high-quality KGs through an automated approach, while the secondary aim is to compare the performance of three leading LLMs—GPT4, LLaMA 2 (13B), and BERT—in extracting meaningful relationships and entities. Unlike conventional methods, which rely on predefined relationship templates, we focus on the ability of these models to infer relationships directly from raw text.  

Due to GPU constraints, our analysis uses a small excerpt from the Wikipedia page on the C programming language as the primary data source for KG creation. This excerpt, while limited in size, provides a diverse range of technical relationships and entities, offering an ideal test case for evaluating the models. By generating KGs from this excerpt, we aim to evaluate not just the accuracy of the relationships inferred but also the semantic coherence of the graphs produced.  

This paper introduces an evaluation framework that goes beyond traditional accuracy measures to include metrics like Graph Edit Distance and Semantic Similarity. By doing so, we provide a comprehensive assessment of how effectively these models can translate unstructured data into structured knowledge. The results of this study highlight the capabilities and limitations of GPT-4, LLaMA 2, and BERT, providing insights into their suitability for automating KG generation. Ultimately, this research lays the groundwork for developing scalable and accurate methods for creating KGs, essential for advancing the utility of GraphRAGs and similar systems.  

# II. R ELATED  W ORK  

The integration of Large Language Models (LLMs) and Knowledge Graphs (KGs) has emerged as a critical area of study, addressing limitations in both technologies while enhancing their mutual functionalities. This section reviews key findings and methodologies from prior research to contextualize our proposed approach.  

# A. Leveraging KGs to Enhance LLMs  

Knowledge graphs contribute to LLMs by grounding their outputs in structured, verified data, mitigating issues such as hallucinations and lack of domain-specific knowledge. Baek et al. introduced KAPING, a method for augmenting LLM prompts with KG-derived facts, enabling zero-shot question answering. Other works, like Knowledge Solver, use KGs  
![](https://cdn-mineru.openxlab.org.cn/model-mineru/prod/286865416d4e47d7df02a4710f0710761f48abc14697f1536969756f0369a568.jpg)  
Fig. 1. Workflow of the pipeline  

to enhance LLM reasoning capabilities through multi-hop inference processes.  

# B. Using LLMs to Build and Improve KGs  

LLMs have been employed to streamline KG construction, particularly from unstructured data. Techniques such as BertNet extract entities and relations directly using paraphrased prompts, while semi-automated pipelines like AutoRD target domain-specific KGs for healthcare and other fields. These methods enable rapid, cost-effective KG generation without extensive manual input (Ibrahim et al.).  

# C. Hybrid Integration Approaches  

Recent studies highlight hybrid approaches that combine the implicit knowledge of LLMs with the explicit structure of KGs. For example, models like ERNIE and KnowBERT jointly embed textual and graph data, improving semantic understanding and enhancing tasks such as entity typing and relation classification. Such integration s demonstrate improved performance in both reasoning and interpret ability (Kau et al.; Ibrahim et al.)  

# III. M ETHODOLOGY  

This section describes the systematic approach used to generate and evaluate knowledge graphs (KGs) using GPT4, LLaMA 2 (13B), and BERT. The aim is to automate KG creation and assess the models’ performance using a standardized evaluation framework. The methodology consists of seven well-defined steps. Fig. 1 depicts a flowchart summarizing the workflow of the pipeline.  

# A. Data Selection  

A small excerpt from the Wikipedia page on the C programming language was selected as the primary data source. This excerpt includes concise descriptions of C’s features, characteristics, and historical context. It provides sufficient relational information to test KG generation while remaining computationally feasible under GPU constraints. The dataset’s size and technical nature make it an ideal choice for this experiment.  

# B. Data Preprocessing  

The text was pre processed to ensure compatibility across the three models. Preprocessing involved:  

Token iz ation : Splitting the text into smaller units, such • as words or phrases, to align with the models’ input requirements.  Cleaning : Removing unnecessary symbols, formatting • errors, and extraneous text that might interfere with relationship extraction.  Formatting : Structuring the input in a uniform format to • avoid introducing bias in how each model processes the text. This step ensures the input text is consistent across all experiments.  

# C. Knowledge Graph Generation  

Each model was tasked with extracting entities and their relationships from the pre processed text. The process involved:  

Entity Recognition : Identifying key terms (e.g., “C • Programming Language,” “Unix”) as nodes in the graph.  Relationship Extraction : Detecting relational links (e.g., • “Derived From,” “Supports”) between these entities.  

The output was structured into a KG format, where nodes represent entities and edges denote relationships. Each model generated its KG independently based on its inherent capabilities.  

# D. Ground Truth Creation  

A manually validated ground truth KG was constructed using the same excerpt. Human expertise was used to accurately identify the entities and relationships present in the text. This ground truth graph serves as the benchmark against which the generated KGs were evaluated.  

# E. Evaluation Metrics  

Five error metrics were employed to evaluate the generated KGs:  

Precision : The proportion of correctly identified relation• ships out of all relationships predicted by the model. True Positives (TP) Precision  $=$  True Positives (TP)  +  False Positives (FP)  Recall : The proportion of actual relationships correctly • identified by the model.  $\mathrm{Recall}={\frac{\mathrm{True~positives~(TP)}}{\mathrm{True~positives~(TP)}+\mathrm{False~Nessit~(FN)}}}$  
F1-Score : The harmonic mean of Precision and Recall, • providing a balanced performance measure.  

$$
\mathrm{F1-score}=2\cdot{\frac{\mathrm{Precision}\cdot{\mathrm{Recall}}}{\mathrm{Precision}+{\mathrm{Recall}}}}
$$  

Graph Edit Distance (GED) : A structural comparison • of the generated graph with the ground truth, measuring the number of edits required to make them identical.  

$$
{\mathrm{GED}}=\sum_{i=1}^{n}{\mathrm{Edt~Omega}}(G_{\mathrm{generated}},G_{\mathrm{ground\truth}})
$$  

Semantic Similarity : A measure of how semantically • close the relationships in the generated KGs are to the relationships in the ground truth, often computed using cosine similarity.  

$$
\frac{\sum_{i=1}^{n}{\mathrm{Vector}}(i)\cdot{\mathrm{Ground~Truth~Vector}}(i)}{||{\mathrm{Vector}}||\cdot||{\mathrm{Ground~Truth~Vector}}||}
$$  

F. Model Comparison  

Each model’s KG was compared against the ground truth using the evaluation metrics. This step involved detailed performance analysis to determine:  

Accuracy of relationships extracted. •  Coverage of entities present in the source text. •  Structural fidelity and semantic alignment of the graph. •  

# IV. R ESULTS AND  D ISCUSSION  

This section presents the performance evaluation of the three models—GPT-4, LLaMA 2, and BERT—using the metrics defined earlier: Precision, Recall, F1-Score, Graph Edit Distance (GED), and Semantic Similarity. Additionally, the knowledge graphs generated by each model are visualized to provide a clearer understanding of their structural and semantic characteristics.  

# A. Visual Comparison of Knowledge Graphs  

Figure 2, 3 and 4 represent the knowledge graphs generated by the three models, compared against the manually curated ground truth graph:  

Each graph showcases the relationships and entities extracted from the dataset. Visually, GPT-4’s graph demonstrates greater structural completeness and alignment with the ground truth, while LLaMA 2 and BERT display some degree of misalignment and missing elements.  

# B. Precision, Recall, and F1-Score  

The  Precision, Recall, and F1-Score  metrics evaluate how accurately each model identifies entity-relationship pairs. Higher scores indicate better alignment with the ground truth.  

TABLE I P RECISION , R ECALL ,  AND  F1-S CORE OF  M ODELS 
![](https://cdn-mineru.openxlab.org.cn/model-mineru/prod/d7163f736f03b518716de049bc8c9fc44acf9fd1deb08798bf8606a873245937.jpg)  

![](https://cdn-mineru.openxlab.org.cn/model-mineru/prod/a72c62f68b0c7ab8555ceba2251f5bfc427154ff7c4d0e93b643a899b590040a.jpg)  
Fig. 2. GPT-4 Generated Knowledge Graph  

![](https://cdn-mineru.openxlab.org.cn/model-mineru/prod/4084b85e974dc52b6d2a879b7d07fcce36d4ceebfb1526a85b0622f092552ead.jpg)  
Fig. 3. LLaMA 2 Generated Knowledge Graph  

As shown in Table 1, GPT-4 outperforms both LLaMA 2 and BERT in all three metrics, with an F1-Score of 0.82. LLaMA 2 exhibits moderate performance with an F1-Score of 0.77, while BERT trails with a lower F1-Score of 0.72.  

# C. Graph Edit Distance (GED)  

Graph Edit Distance  measures the number of node and edge transformations required to convert the generated graph into the ground truth graph. Lower GED values indicate better graph structural similarity. The GED scores are indicated in Table 2.  

TABLE II G RAPH  E DIT  D ISTANCE OF  M ODELS 
![](https://cdn-mineru.openxlab.org.cn/model-mineru/prod/bcca0bdfd2508f3135df0a9e1eaec52bdd9745b7537324beaa82ec6b361de9ea.jpg)  

The results highlight that GPT-4 requires the fewest transformations (6), indicating it generates graphs structurally closer to the ground truth. LLaMA 2 and BERT, with GED values of 8 and 10 respectively, show a larger deviation.  
![](https://cdn-mineru.openxlab.org.cn/model-mineru/prod/8e13d06d1545834a67590d96b0770f676688a94e68144b36d20d902d30ac8352.jpg)  
Fig. 4. BERT Generated Knowledge Graph  

# D. Semantic Similarity  

Semantic Similarity  evaluates how well the entities and rela tion ships in the generated graphs semantically align with the ground truth using cosine similarity. The Semantic Simlarity scores are indicated in Table 3.  

TABLE III S EMANTIC  S IMILARITY OF  M ODELS 
![](https://cdn-mineru.openxlab.org.cn/model-mineru/prod/e68bf27527ce05e574244af0ae0209c2766c5277c3fa5007d425f86753f40f88.jpg)  

GPT-4 achieves the highest overall similarity (0.87), reflecting its superior understanding of both entities and relationships. LLaMA 2 and BERT follow with scores of 0.82 and 0.77, respectively.  

# E. Discussion  

The results clearly demonstrate that GPT-4 is the most capable model among the three for automated knowledge graph generation, achieving the highest scores across all metrics. Its better semantic understanding and structural alignment with the ground truth make it a strong candidate for tasks involving GraphRAG systems. The visual comparison of knowledge graphs further supports these findings, with GPT-4’s graph showing the highest structural and semantic fidelity. LLaMA 2 provides a balance between performance and resource constraints, while BERT lags behind, primarily due to its limited contextual understanding in this task.  

# V. C ONCLUSION  

The task of automating knowledge graph generation is crucial for the advancement of GraphRAG systems, which rely heavily on accurate and meaningful knowledge representations. In this study, we proposed and evaluated a methodology  that leverages large language models (LLMs) such as GPT-4, LLaMA 2, and BERT to generate knowledge graphs directly from unstructured data. This approach addresses the limitations of traditional methods, such as relationship classification, which often require extensive manual input and lack s cal ability for large datasets.  

Our findings indicate that GPT-4 consistently outperforms the other models, both in terms of semantic and structural alignment with the ground truth knowledge graph. The combination of higher precision, recall, and semantic similarity metrics, alongside a lower graph edit distance, highlights GPT4’s capability to generate knowledge graphs that are both accurate and coherent. While LLaMA 2 showed moderate effectiveness, its performance suggests potential for use in scenarios with resource constraints, where the trade-off between computational efficiency and accuracy is acceptable. On the other hand, BERT, despite its foundational role in NLP, struggles to handle the complexities of this task, particularly in generating relationships that capture the nuances of the input data.  

Beyond comparing the models, this research underscores the feasibility of leveraging LLMs for automated knowledge graph creation. By using a simplified dataset, we demonstrated that even with minimal resources, meaningful insights can be derived. Future research can extend this work by testing on larger, more complex datasets and incorporating domainspecific adaptations to refine graph accuracy further.  

This study highlights the importance of continuing advancements in LLMs to make knowledge graph generation more accessible and scalable, particularly for applications in information retrieval, reasoning, and decision-making. The methodology and findings pave the way for developing more efficient and accurate systems, reducing reliance on manual processes, and enabling the broader adoption of GraphRAGs in real-world applications.  

# R EFERENCES  

[1] Kau, A., He, X., Nambissan, A., Astudillo, A., Yin, H., and Aryani, A. (2024). Combining Knowledge Graphs and Large Language Models. ArXiv .

[2] Ibrahim, N., Aboulela, S., Ibrahim, A. et al. A survey on augmenting knowledge graphs (KGs) with large language models (LLMs): models, evaluation metrics, benchmarks, and challenges. Discov Artif Intell 4, 76 (2024). The inclusion of these visualization s enhances the interpre t ability of the results, emphasizing the critical role of advanced LLMs in addressing the challenges of automated KG generation for GraphRAGs.  
